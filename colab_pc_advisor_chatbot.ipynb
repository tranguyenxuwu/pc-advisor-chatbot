{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PC Advisor RAG Chatbot (Google Colab)\n",
        "\n",
        "This notebook prepares the retrieval-augmented PC advisor chatbot in a Google Colab runtime. It installs the required\n",
        "packages, builds the ChromaDB vector store, loads the embedding and reranker models, and finally launches a Gradio chat\n",
        "interface that talks to any OpenAI-compatible LLM endpoint. A final cell also shows how to call the pipeline directly\n",
        "from Python code.\n",
        "\n",
        "> **Important:** You must supply your own API key and base URL for an OpenAI-compatible endpoint (Groq, Together, OpenAI, etc.).\n",
        "> Do **not** share the notebook after adding secrets unless you remove them first.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/pc-advisor-chatbot\" if IN_COLAB else \".\").resolve()\n",
        "\n",
        "if IN_COLAB:\n",
        "    if not PROJECT_ROOT.exists():\n",
        "        # TODO: Replace the placeholder below with the actual GitHub URL of your fork if needed.\n",
        "        REPO_URL = \"https://github.com/YOUR_ACCOUNT/pc-advisor-chatbot.git\"\n",
        "        if \"YOUR_ACCOUNT\" in REPO_URL:\n",
        "            raise ValueError(\n",
        "                \"Please set REPO_URL to the GitHub HTTPS URL of your pc-advisor-chatbot repository before running this cell.\"\n",
        "            )\n",
        "        subprocess.run([\"git\", \"clone\", REPO_URL, str(PROJECT_ROOT)], check=True)\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "else:\n",
        "    if not (PROJECT_ROOT / \"app.py\").exists():\n",
        "        raise RuntimeError(\n",
        "            \"Run this notebook from the root of the pc-advisor-chatbot repository or enable the Colab cloning block above.\"\n",
        "        )\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "\n",
        "print(f\"Working directory: {Path.cwd()}\")\n",
        "print(\"Repository files:\")\n",
        "print(\"\n",
        "\".join(sorted(str(p.relative_to(Path.cwd())) for p in Path.cwd().iterdir())))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install dependencies (this may take a few minutes the first time).\n",
        "!pip install -q -r requirements.txt gradio==4.44.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Build the ChromaDB vector store if it has not been created yet.\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "chroma_path = Path(\"data/chromadb\")\n",
        "if chroma_path.exists():\n",
        "    print(\"ChromaDB directory already exists \u2013 skipping embedding generation.\")\n",
        "else:\n",
        "    print(\"Creating embeddings and populating ChromaDB (this can take several minutes)...\")\n",
        "    subprocess.run([sys.executable, \"embedding.py\"], check=True)\n",
        "    print(\"Embedding build complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load retrieval components (embedding model + reranker).\n",
        "import retrieval\n",
        "\n",
        "device = retrieval.setup_device()\n",
        "embedding_model = retrieval.load_embedding_model()\n",
        "reranker_data = retrieval.load_reranker_data(device)\n",
        "print(\"Models loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import openai\n",
        "import gradio as gr\n",
        "import re\n",
        "from typing import Dict, Generator, List, Optional\n",
        "\n",
        "\n",
        "def parse_response(response: str):\n",
        "    \"\"\"Split the LLM response into hidden thoughts and the visible answer.\"\"\"\n",
        "    think_pattern = r\"<think>(.*?)</think>\"\n",
        "    match = re.search(think_pattern, response, re.DOTALL)\n",
        "    if match:\n",
        "        thought_content = match.group(1).strip()\n",
        "        clean_response = re.sub(think_pattern, \"\", response, count=1, flags=re.DOTALL).strip()\n",
        "        return thought_content, clean_response\n",
        "    return None, response.strip()\n",
        "\n",
        "\n",
        "def get_openai_client(api_key: str, base_url: str) -> openai.OpenAI:\n",
        "    if not api_key or not base_url:\n",
        "        raise ValueError(\"Both API key and base URL are required.\")\n",
        "    return openai.OpenAI(api_key=api_key, base_url=base_url)\n",
        "\n",
        "\n",
        "def rewrite_query_with_llm(client: openai.OpenAI, query: str, model: str) -> str:\n",
        "    if not client:\n",
        "        return query\n",
        "    system_prompt = (\n",
        "        \"You are an expert AI assistant that rewrites user queries for a vector database search. \"\n",
        "        \"The database contains two types of documents: \"\n",
        "        \"1. Individual PC components (CPU, GPU/VGA, RAM, SSD, Mainboard, PSU, Case, etc.). \"\n",
        "        \"2. Pre-built or assembled PCs (ready-to-use systems). \"\n",
        "        \"Your task is to analyze the user's natural-language query and rewrite it into a concise, \"\n",
        "        \"keyword-rich search query that best captures the user's intent.\n",
        "\n",
        "\"\n",
        "        \"Guidelines:\n",
        "\"\n",
        "        \"- If the user wants to build or assemble a PC, focus on component-level keywords and budget splits.\n",
        "\"\n",
        "        \"- Dedicate roughly 80% of the budget to CPU/GPU/RAM/SSD and the rest to supporting parts.\n",
        "\"\n",
        "        \"- Provide approximate per-component prices when the total budget is known.\n",
        "\"\n",
        "        \"- If the user wants a pre-built PC, use keywords like 'pre-built PC' along with the target use case.\n",
        "\"\n",
        "        \"- Expand vague terms into precise hardware-related keywords.\n",
        "\"\n",
        "        \"- Include both Vietnamese and English keywords if helpful.\n",
        "\"\n",
        "        \"- All currency references are in VND.\n",
        "\"\n",
        "        \"Do not answer the question \u2013 only output the rewritten search query.\"\n",
        "    )\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": f\"Rewrite the following query: '{query}'\"},\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "        )\n",
        "        rewritten_query = response.choices[0].message.content.strip()\n",
        "        return rewritten_query.replace('\"', \"\")\n",
        "    except Exception as exc:  # noqa: BLE001\n",
        "        print(f\"Rewriting failed ({exc}); using original query instead.\")\n",
        "        return query\n",
        "\n",
        "\n",
        "def generate_response_stream(\n",
        "    client: openai.OpenAI,\n",
        "    messages: List[Dict[str, str]],\n",
        "    retrieved_info: str,\n",
        "    model: str,\n",
        ") -> Generator[str, None, None]:\n",
        "    if not client:\n",
        "        yield \"Error: OpenAI client not initialized.\"\n",
        "        return\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are a helpful Vietnamese assistant for a PC parts store. \"\n",
        "        \"Use the retrieved information below to answer the user's latest question. \"\n",
        "        \"Be concise, natural, and friendly.\n",
        "\n",
        "\"\n",
        "        f\"Retrieved Information:\n",
        "{retrieved_info}\"\n",
        "    )\n",
        "    final_messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
        "\n",
        "    try:\n",
        "        stream = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=final_messages,\n",
        "            stream=True,\n",
        "        )\n",
        "        for chunk in stream:\n",
        "            content = chunk.choices[0].delta.content\n",
        "            if content:\n",
        "                yield content\n",
        "    except Exception as exc:  # noqa: BLE001\n",
        "        yield f\"Error generating response: {exc}\"\n",
        "\n",
        "\n",
        "def format_history_for_llm(history: List[List[str]]) -> List[Dict[str, str]]:\n",
        "    formatted: List[Dict[str, str]] = []\n",
        "    for user_message, assistant_message in history:\n",
        "        formatted.append({\"role\": \"user\", \"content\": user_message})\n",
        "        if assistant_message:\n",
        "            formatted.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    return formatted\n",
        "\n",
        "\n",
        "def rag_chat(message: str, history: List[List[str]], api_key: str, base_url: str, model: str):\n",
        "    \"\"\"Gradio-compatible chat function that performs retrieval + generation.\"\"\"\n",
        "    if not api_key or not base_url:\n",
        "        raise gr.Error(\"Please provide both the API key and the base URL for your LLM endpoint.\")\n",
        "\n",
        "    client = get_openai_client(api_key, base_url)\n",
        "\n",
        "    rewritten_query = rewrite_query_with_llm(client, message, model)\n",
        "    retrieved_info = retrieval.perform_retrieval_and_reranking(\n",
        "        rewritten_query,\n",
        "        embedding_model,\n",
        "        reranker_data,\n",
        "    )\n",
        "\n",
        "    messages = format_history_for_llm(history)\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    collected_response = \"\"\n",
        "    for chunk in generate_response_stream(client, messages, retrieved_info, model):\n",
        "        collected_response += chunk\n",
        "\n",
        "    thoughts, clean_response = parse_response(collected_response)\n",
        "    debug_payload: Dict[str, Optional[str]] = {\n",
        "        \"rewritten_query\": rewritten_query,\n",
        "        \"retrieved_context\": retrieved_info,\n",
        "    }\n",
        "    if thoughts:\n",
        "        debug_payload[\"llm_thoughts\"] = thoughts\n",
        "\n",
        "    return clean_response or collected_response, debug_payload\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## PC Advisor RAG Chatbot\")\n",
        "    gr.Markdown(\n",
        "        \"Provide your OpenAI-compatible API credentials below, then start chatting. \"\n",
        "        \"Use the JSON panel to inspect the rewritten query and the retrieved context.\"\n",
        "    )\n",
        "    chat_interface = gr.ChatInterface(\n",
        "        fn=rag_chat,\n",
        "        additional_inputs=[\n",
        "            gr.Textbox(label=\"API Key\", type=\"password\", placeholder=\"sk-...\"),\n",
        "            gr.Textbox(label=\"Base URL\", value=\"http://127.0.0.1:1234/v1\"),\n",
        "            gr.Textbox(label=\"Model\", value=\"qwen/qwen3-4b-thinking-2507\"),\n",
        "        ],\n",
        "        additional_outputs=gr.JSON(label=\"RAG Debug Info\"),\n",
        "        title=\"PC Advisor Chatbot\",\n",
        "        description=\"Ask about PC parts or pre-built systems in Vietnamese or English.\",\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional: call the pipeline programmatically from Python.\n",
        "# Fill in your credentials before running.\n",
        "API_KEY = \"\"\n",
        "BASE_URL = \"http://127.0.0.1:1234/v1\"  # Update if you are using a remote endpoint\n",
        "MODEL_NAME = \"qwen/qwen3-4b-thinking-2507\"\n",
        "SAMPLE_QUESTION = \"T\u00f4i c\u00f3 25 tri\u1ec7u, t\u01b0 v\u1ea5n c\u1ea5u h\u00ecnh PC ch\u01a1i game v\u00e0 l\u00e0m \u0111\u1ed3 h\u1ecda nh\u1eb9?\"\n",
        "\n",
        "if API_KEY and BASE_URL:\n",
        "    client = get_openai_client(API_KEY, BASE_URL)\n",
        "    rewritten = rewrite_query_with_llm(client, SAMPLE_QUESTION, MODEL_NAME)\n",
        "    print(\"Rewritten query:\n",
        "\", rewritten)\n",
        "\n",
        "    context = retrieval.perform_retrieval_and_reranking(\n",
        "        rewritten,\n",
        "        embedding_model,\n",
        "        reranker_data,\n",
        "    )\n",
        "    print(\"\n",
        "Retrieved context:\n",
        "\", context)\n",
        "\n",
        "    response_text = \"\".join(\n",
        "        generate_response_stream(\n",
        "            client,\n",
        "            [{\"role\": \"user\", \"content\": SAMPLE_QUESTION}],\n",
        "            context,\n",
        "            MODEL_NAME,\n",
        "        )\n",
        "    )\n",
        "    thoughts, final_answer = parse_response(response_text)\n",
        "    if thoughts:\n",
        "        print(\"\n",
        "LLM thoughts:\n",
        "\", thoughts)\n",
        "    print(\"\n",
        "Assistant answer:\n",
        "\", final_answer or response_text)\n",
        "else:\n",
        "    print(\"Set API_KEY and BASE_URL above to test the pipeline programmatically.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}