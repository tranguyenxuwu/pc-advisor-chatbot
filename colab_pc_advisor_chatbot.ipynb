{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PC Advisor RAG Chatbot (Google Colab)\n",
        "\n",
        "This notebook prepares the retrieval-augmented PC advisor chatbot in a Google Colab runtime. It installs the required packages, builds the ChromaDB vector store, loads the embedding and reranker models, and finally launches a Gradio chat interface that runs entirely on the Colab GPU using an open-source LLM\u2014no external API key required. A final cell also shows how to call the pipeline directly from Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/pc-advisor-chatbot\" if IN_COLAB else \".\").resolve()\n",
        "\n",
        "if IN_COLAB:\n",
        "    if not PROJECT_ROOT.exists():\n",
        "        # TODO: Replace the placeholder below with the actual GitHub URL of your fork if needed.\n",
        "        REPO_URL = \"https://github.com/YOUR_ACCOUNT/pc-advisor-chatbot.git\"\n",
        "        if \"YOUR_ACCOUNT\" in REPO_URL:\n",
        "            raise ValueError(\n",
        "                \"Please set REPO_URL to the GitHub HTTPS URL of your pc-advisor-chatbot repository before running this cell.\"\n",
        "            )\n",
        "        subprocess.run([\"git\", \"clone\", REPO_URL, str(PROJECT_ROOT)], check=True)\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "else:\n",
        "    if not (PROJECT_ROOT / \"app.py\").exists():\n",
        "        raise RuntimeError(\n",
        "            \"Run this notebook from the root of the pc-advisor-chatbot repository or enable the Colab cloning block above.\"\n",
        "        )\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "\n",
        "print(f\"Working directory: {Path.cwd()}\")\n",
        "print(\"Repository files:\")\n",
        "print(\"\n",
        "\".join(sorted(str(p.relative_to(Path.cwd())) for p in Path.cwd().iterdir())))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install dependencies (this may take a few minutes the first time).\n",
        "!pip install -q -U pip\n",
        "!pip install -q -r requirements.txt gradio==4.44.0 transformers accelerate bitsandbytes sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Build the ChromaDB vector store if it has not been created yet.\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "chroma_path = Path(\"data/chromadb\")\n",
        "if chroma_path.exists():\n",
        "    print(\"ChromaDB directory already exists \u2013 skipping embedding generation.\")\n",
        "else:\n",
        "    print(\"Creating embeddings and populating ChromaDB (this can take several minutes)...\")\n",
        "    subprocess.run([sys.executable, \"embedding.py\"], check=True)\n",
        "    print(\"Embedding build complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load retrieval components (embedding model + reranker).\n",
        "import retrieval\n",
        "\n",
        "device = retrieval.setup_device()\n",
        "embedding_model = retrieval.load_embedding_model()\n",
        "reranker_data = retrieval.load_reranker_data(device)\n",
        "print(\"Models loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import Dict, List, Optional\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "GENERATION_MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "\n",
        "def _select_best_dtype() -> torch.dtype:\n",
        "    if torch.cuda.is_available():\n",
        "        major, _ = torch.cuda.get_device_capability()\n",
        "        if major >= 8:\n",
        "            return torch.bfloat16\n",
        "        return torch.float16\n",
        "    return torch.float32\n",
        "\n",
        "\n",
        "print(f\"Loading generation model '{GENERATION_MODEL_NAME}'...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(GENERATION_MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    GENERATION_MODEL_NAME,\n",
        "    torch_dtype=_select_best_dtype(),\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def parse_response(response: str):\n",
        "    \"\"\"Split the LLM response into hidden thoughts and the visible answer.\"\"\"\n",
        "    import re\n",
        "\n",
        "    think_pattern = r\"<think>(.*?)</think>\"\n",
        "    match = re.search(think_pattern, response, re.DOTALL)\n",
        "    if match:\n",
        "        thought_content = match.group(1).strip()\n",
        "        clean_response = re.sub(think_pattern, \"\", response, count=1, flags=re.DOTALL).strip()\n",
        "        return thought_content, clean_response\n",
        "    return None, response.strip()\n",
        "\n",
        "\n",
        "def _generate_with_local_model(\n",
        "    messages: List[Dict[str, str]],\n",
        "    *,\n",
        "    temperature: float,\n",
        "    max_new_tokens: int,\n",
        ") -> str:\n",
        "    chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(chat_prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    generation_kwargs = dict(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=temperature > 0.0,\n",
        "        temperature=max(temperature, 1e-5) if temperature > 0.0 else 0.0,\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**generation_kwargs)\n",
        "\n",
        "    generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def rewrite_query_with_llm(query: str) -> str:\n",
        "    system_prompt = (\n",
        "        \"You are an expert AI assistant that rewrites user queries for a vector database search. \"\n",
        "        \"The database contains two types of documents: \"\n",
        "        \"1. Individual PC components (CPU, GPU/VGA, RAM, SSD, Mainboard, PSU, Case, etc.). \"\n",
        "        \"2. Pre-built or assembled PCs (ready-to-use systems). \"\n",
        "        \"Your task is to analyze the user's natural-language query and rewrite it into a concise, \"\n",
        "        \"keyword-rich search query that best captures the user's intent.\n",
        "\n",
        "\"\n",
        "        \"Guidelines:\n",
        "\"\n",
        "        \"- If the user wants to build or assemble a PC, focus on component-level keywords and budget splits.\n",
        "\"\n",
        "        \"- Dedicate roughly 80% of the budget to CPU/GPU/RAM/SSD and the rest to supporting parts.\n",
        "\"\n",
        "        \"- Provide approximate per-component prices when the total budget is known.\n",
        "\"\n",
        "        \"- If the user wants a pre-built PC, use keywords like 'pre-built PC' along with the target use case.\n",
        "\"\n",
        "        \"- Expand vague terms into precise hardware-related keywords.\n",
        "\"\n",
        "        \"- Include both Vietnamese and English keywords if helpful.\n",
        "\"\n",
        "        \"- All currency references are in VND.\n",
        "\"\n",
        "        \"Do not answer the question \u2013 only output the rewritten search query.\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": query},\n",
        "    ]\n",
        "\n",
        "    response = _generate_with_local_model(messages, temperature=0.0, max_new_tokens=256)\n",
        "    cleaned = response.replace('\"', \"\").strip()\n",
        "    return cleaned.splitlines()[0] if cleaned else query\n",
        "\n",
        "\n",
        "def format_history_for_llm(history: List[List[str]]) -> List[Dict[str, str]]:\n",
        "    formatted: List[Dict[str, str]] = []\n",
        "    for user_message, assistant_message in history:\n",
        "        formatted.append({\"role\": \"user\", \"content\": user_message})\n",
        "        if assistant_message:\n",
        "            formatted.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    return formatted\n",
        "\n",
        "\n",
        "def rag_chat(\n",
        "    message: str,\n",
        "    history: List[List[str]],\n",
        "    temperature: float,\n",
        "    max_new_tokens: int,\n",
        "):\n",
        "    temperature = float(temperature)\n",
        "    max_new_tokens = int(max_new_tokens)\n",
        "\n",
        "    rewritten_query = rewrite_query_with_llm(message)\n",
        "    retrieved_info = retrieval.perform_retrieval_and_reranking(\n",
        "        rewritten_query,\n",
        "        embedding_model,\n",
        "        reranker_data,\n",
        "    )\n",
        "\n",
        "    history_messages = format_history_for_llm(history)\n",
        "    history_messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are a helpful Vietnamese assistant for a PC parts store. \"\n",
        "        \"Always think through the answer inside <think></think> tags before speaking to the user. \"\n",
        "        \"Use the retrieved context when it is available, cite prices in VND, and keep the final answer concise and friendly. \"\n",
        "        \"If the context is empty, rely on your general PC knowledge and mention that you are doing so.\"\n",
        "    )\n",
        "    contextual_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"{system_prompt}\n",
        "\n",
        "Retrieved context:\n",
        "{retrieved_info or 'No relevant documents were found.'}\",\n",
        "        }\n",
        "    ] + history_messages\n",
        "\n",
        "    collected_response = _generate_with_local_model(\n",
        "        contextual_messages,\n",
        "        temperature=temperature,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "    )\n",
        "\n",
        "    thoughts, clean_response = parse_response(collected_response)\n",
        "    debug_payload: Dict[str, Optional[str]] = {\n",
        "        \"rewritten_query\": rewritten_query,\n",
        "        \"retrieved_context\": retrieved_info,\n",
        "        \"generation_temperature\": f\"{temperature:.2f}\",\n",
        "        \"max_new_tokens\": int(max_new_tokens),\n",
        "    }\n",
        "    if thoughts:\n",
        "        debug_payload[\"llm_thoughts\"] = thoughts\n",
        "\n",
        "    return clean_response or collected_response, debug_payload\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## PC Advisor RAG Chatbot\")\n",
        "    gr.Markdown(\n",
        "        \"The entire retrieval and generation pipeline runs locally on this Colab runtime. \"\n",
        "        \"Adjust the temperature or token limit if you want more creative or longer responses.\"\n",
        "    )\n",
        "    chat_interface = gr.ChatInterface(\n",
        "        fn=rag_chat,\n",
        "        additional_inputs=[\n",
        "            gr.Slider(0.0, 1.0, value=0.2, step=0.05, label=\"Generation Temperature\"),\n",
        "            gr.Slider(128, 1024, value=512, step=64, label=\"Max New Tokens\"),\n",
        "        ],\n",
        "        additional_outputs=gr.JSON(label=\"RAG Debug Info\"),\n",
        "        title=\"PC Advisor Chatbot\",\n",
        "        description=\"Ask about PC parts or pre-built systems in Vietnamese or English.\",\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional: call the pipeline programmatically from Python.\n",
        "SAMPLE_QUESTION = \"T\u00f4i c\u00f3 25 tri\u1ec7u, t\u01b0 v\u1ea5n c\u1ea5u h\u00ecnh PC ch\u01a1i game v\u00e0 l\u00e0m \u0111\u1ed3 h\u1ecda nh\u1eb9?\"\n",
        "\n",
        "answer, debug_info = rag_chat(\n",
        "    SAMPLE_QUESTION,\n",
        "    history=[],\n",
        "    temperature=0.2,\n",
        "    max_new_tokens=512,\n",
        ")\n",
        "\n",
        "print(\"Question:\", SAMPLE_QUESTION)\n",
        "print(\"\\nAnswer:\\n\", answer)\n",
        "print(\"\\nDebug Info:\")\n",
        "for key, value in debug_info.items():\n",
        "    print(f\"- {key}: {value}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}