version: "3.8"

services:
  rag-chatbot:
    depends_on:
      - llm
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag-chatbot
    ports:
      - "8501:8501"
    volumes:
      # Mount data directory to persist ChromaDB data
      - ./data:/app/data
      # Mount cache directory for HuggingFace models
      - huggingface-cache:/root/.cache/huggingface
    environment:
      # Streamlit configuration
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      # Optional: Add your OpenAI API key here or use .env file
      # - OPENAI_API_KEY=${OPENAI_API_KEY}
      # - OPENAI_BASE_URL=${OPENAI_BASE_URL}
    restart: unless-stopped
    # Uncomment below if you have NVIDIA GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    networks:
      - rag-network

  llm:
    provider:
      type: model
      options:
        model: hf.co/lmstudio-community/qwen3-4b-instruct-2507-gguf:q4_k_m
    networks:
      - rag-network

volumes:
  huggingface-cache:
    driver: local

networks:
  rag-network:
    driver: bridge
