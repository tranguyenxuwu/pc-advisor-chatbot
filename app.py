import streamlit as st
import openai
import re
import time

# Import c√°c h√†m c·∫ßn thi·∫øt t·ª´ module retrieval c·ªßa ch√∫ng ta
import retrieval 

# --- CONFIGURATION ---
st.set_page_config(page_title="PC Assistant Chatbot", layout="wide")

# --- UTILITY FUNCTIONS (For UI and OpenAI) ---

def parse_response(response: str):
    """T√°ch ph·∫ßn suy nghƒ© c·ªßa LLM ra kh·ªèi c√¢u tr·∫£ l·ªùi cu·ªëi c√πng."""
    think_pattern = r'<think>(.*?)</think>'
    match = re.search(think_pattern, response, re.DOTALL)
    if match:
        thought_content = match.group(1).strip()
        clean_response = re.sub(think_pattern, '', response, count=1, flags=re.DOTALL).strip()
        return thought_content, clean_response
    return None, response.strip()

def get_openai_client(api_key: str, base_url: str):
    """Kh·ªüi t·∫°o OpenAI client."""
    if not api_key or not base_url: return None
    try:
        return openai.OpenAI(api_key=api_key, base_url=base_url)
    except Exception as e:
        st.error(f"Failed to initialize OpenAI client: {e}")
        return None

def rewrite_query_with_llm(client: openai.OpenAI, query: str, model: str):
    """S·ª≠ d·ª•ng LLM ƒë·ªÉ bi·∫øn c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng th√†nh truy v·∫•n t√¨m ki·∫øm t·ªëi ∆∞u."""
    if not client:
        return query # Tr·∫£ v·ªÅ truy v·∫•n g·ªëc n·∫øu client kh√¥ng h·ª£p l·ªá

    system_prompt = (
        "You are an expert AI assistant that rewrites user queries for a vector database search. "
        "The database contains two types of documents: "
        "1. Individual PC components (CPU, GPU/VGA, RAM, SSD, Mainboard, PSU, Case, etc.). "
        "2. Pre-built or assembled PCs (ready-to-use systems). "
        "Your task is to analyze the user's natural-language query and rewrite it into a concise, "
        "keyword-rich search query that best captures the user's intent.\n\n"

        "Guidelines:\n"
        "- If the user wants to **build or assemble** a PC ('x√¢y d·ª±ng', 'build PC', 't·ª± r√°p m√°y'), "
        "focus on component-level keywords such as CPU, GPU, RAM, SSD, and optionally 'mainboard', 'case', 'PSU'.\n"
        "- Assume about **80% of the user's total budget** is allocated for the four main components (CPU, GPU, RAM, SSD), "
        "and the remaining **20%** for supporting parts (mainboard, PSU, case, cooling, etc.).\n"
        "- Within that 80%, estimate detailed allocations based on user intent:\n"
        "   ‚Ä¢ **Gaming PC** ‚Üí GPU 30‚Äì35%, CPU 20‚Äì25%, RAM 7‚Äì10%, SSD 5‚Äì8%.\n"
        "   ‚Ä¢ **Workstation / Rendering PC** ‚Üí CPU 30‚Äì35%, GPU 25‚Äì30%, RAM 7‚Äì10%, SSD 5‚Äì8%.\n"
        "   ‚Ä¢ **Office / General-use PC** ‚Üí CPU 25‚Äì30%, GPU 20‚Äì25%, RAM 7‚Äì10%, SSD 5‚Äì8%.\n"
        "- If the total budget is mentioned (e.g. 'PC 30 tri·ªáu'), estimate approximate per-component prices and include them in the rewritten query, "
        "e.g. 'CPU 5 tri·ªáu, GPU 10 tri·ªáu, RAM 3 tri·ªáu, SSD 2 tri·ªáu'.\n"
        "- If the user wants a **pre-built PC** ('m√°y t√≠nh l·∫Øp s·∫µn', 'PC vƒÉn ph√≤ng', 'PC gaming s·∫µn'), "
        "focus on keywords like 'pre-built PC', 'ready-to-use', and include the target use ('gaming', 'office', 'editing', etc.).\n"
        "- Expand vague terms into precise hardware-related keywords (e.g., 'm√°y m·∫°nh' ‚Üí 'high-performance gaming PC', "
        "'m√°y render' ‚Üí 'workstation PC with strong CPU and GPU').\n"
        "- Include both Vietnamese and English keywords if helpful for better retrieval.\n"
        "all mentioned and retrived currency is VND (Vietnamese Dong).\n"
        "- Do **NOT** answer the question or explain ‚Äî only output the rewritten search query."
    )
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Rewrite the following query: '{query}'"}
            ],
            temperature=0.0
        )
        rewritten_query = response.choices[0].message.content.strip()
        return rewritten_query.replace('"', '')
    except openai.APIError:
        return query

def generate_response_stream(client: openai.OpenAI, messages: list, retrieved_info: str, model: str):
    """Yields response chunks from the LLM API stream."""
    if not client:
        yield "Error: OpenAI client not initialized."
        return

    try:
        # <<< THAY ƒê·ªîI 1: S·ª¨A L·ªñI F-STRING >>>
        # ƒê√£ x√≥a c·∫∑p ngo·∫∑c nh·ªçn th·ª´a xung quanh `retrieved_info`.
        # TR∆Ø·ªöC ƒê√ÇY: f"Retrieved Information:\n{{retrieved_info}}"
        # B√ÇY GI·ªú:   f"Retrieved Information:\n{retrieved_info}"
        system_prompt = (
            "You are a helpful Vietnamese assistant for a PC parts store. "
            "Use the retrieved information below to answer the user's latest question. "
            "Be concise, natural, and friendly.\n\n"
            f"Retrieved Information:\n{retrieved_info}"
        )
        final_messages = [{"role": "system", "content": system_prompt}] + messages

        stream = client.chat.completions.create(
            model=model,
            messages=final_messages,
            stream=True
        )

        for chunk in stream:
            content = chunk.choices[0].delta.content
            if content:
                yield content
    except openai.APIError as e:
        yield f"Error generating response: {e}"

# --- MODEL LOADING (with Streamlit Caching) ---

@st.cache_resource(show_spinner="Setting up device...")
def cached_setup_device():
    return retrieval.setup_device()

@st.cache_resource(show_spinner="Loading embedding model (Qwen-Embedding)...")
def cached_load_embedding_model():
    return retrieval.load_embedding_model()

@st.cache_resource(show_spinner="Loading reranker model (Qwen-Reranker)...")
def cached_load_reranker_data(device):
    return retrieval.load_reranker_data(device)

# --- STREAMLIT UI ---
st.title("PC Assistant Chatbot üí¨")

device = cached_setup_device()
embedding_model = cached_load_embedding_model()
reranker_data = cached_load_reranker_data(device)

with st.sidebar:
    st.header("‚öôÔ∏è LLM Configuration")
    st.info("Configure the API endpoint for your **generation** LLM below.")
    
    generation_api = st.text_input("API Endpoint Base URL", value="http://127.0.0.1:1234/v1")
    api_key = st.text_input("API Key", type="password", value="not-needed")
    client = get_openai_client(api_key, generation_api)
    selected_model = st.text_input("Select Generation Model", value="qwen/qwen3-4b-thinking-2507")

if "messages" not in st.session_state:
    st.session_state.messages = []

# <<< THAY ƒê·ªîI 2: C·∫¨P NH·∫¨T GIAO DI·ªÜN L·ªäCH S·ª¨ TR√í CHUY·ªÜN >>>
# S·ª≠ d·ª•ng st.expander ƒë·ªÉ hi·ªÉn th·ªã kh·ªëi <think> trong l·ªãch s·ª≠.
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if message["role"] == "assistant" and "thoughts" in message and message["thoughts"]:
            with st.expander("Show Thought Process"):
                st.markdown(message["thoughts"])
        st.markdown(message["content"])

if prompt := st.chat_input("H·ªèi v·ªÅ linh ki·ªán ho·∫∑c PC d·ª±ng s·∫µn..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Ph√¢n t√≠ch v√† t·ªëi ∆∞u h√≥a c√¢u h·ªèi..."):
            rewritten_query = rewrite_query_with_llm(client, prompt, selected_model)
            with st.expander("Show Optimized Query for Retrieval"):
                st.info(f"**Original:** {prompt}\n\n**Optimized:** {rewritten_query}")

        with st.spinner("ƒêang t√¨m ki·∫øm v√† x·∫øp h·∫°ng th√¥ng tin..."):
            retrieved_info = retrieval.perform_retrieval_and_reranking(rewritten_query, embedding_model, reranker_data)
            with st.expander("Show Reranked Context"):
                st.info(retrieved_info or "No context found.")
        
        if not client:
            st.warning("Cannot generate response. LLM API client not configured.")
        else:
            # <<< THAY ƒê·ªîI 3: C·∫¨P NH·∫¨T LU·ªíNG STREAMING V·ªöI DROPDOWN >>>
            # S·ª≠ d·ª•ng placeholder cho c√¢u tr·∫£ l·ªùi v√† t·∫°o expander khi c√≥ n·ªôi dung suy nghƒ©.
            answer_placeholder = st.empty()
            
            full_raw_response = ""
            thought_content = ""
            clean_response = ""
            
            is_thinking_parsed = False

            response_stream = generate_response_stream(client, st.session_state.messages, retrieved_info, model=selected_model)

            # T·∫°o expander tr∆∞·ªõc, nh∆∞ng ch∆∞a ƒëi·ªÅn n·ªôi dung.
            thought_expander = st.expander("Show Thought Process")
            
            for chunk in response_stream:
                full_raw_response += chunk
                
                if "</think>" in full_raw_response and not is_thinking_parsed:
                    temp_thought, temp_clean = parse_response(full_raw_response)
                    if temp_thought:
                        thought_content = temp_thought
                        clean_response = temp_clean
                        
                        # ƒêi·ªÅn n·ªôi dung v√†o expander ƒë√£ t·∫°o
                        thought_expander.markdown(thought_content)
                        
                        answer_placeholder.markdown(clean_response)
                        is_thinking_parsed = True
                elif is_thinking_parsed:
                    clean_response += chunk
                    answer_placeholder.markdown(clean_response)
                else:
                    answer_placeholder.markdown(full_raw_response)
            
            _, final_clean_response = parse_response(full_raw_response)

            st.session_state.messages.append({
                "role": "assistant",
                "content": final_clean_response,
                "thoughts": thought_content
            })